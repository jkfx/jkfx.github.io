<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 关于 RNN 循环神经网络的反向传播求导 | Geek F. x </title> <meta name="author" content="Geek F. x"> <meta name="description" content=""> <meta name="keywords" content="academic-website, portfolio-website, geek, geekfx, geekjkfx, jkfx, blog, computer-science, deep-learning, AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon/favicon_blue/favicon.ico?88a93ea07c0a82656ab29388cee125af"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jkfx.github.io/blog/2021/%E5%85%B3%E4%BA%8E-RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B1%82%E5%AF%BC/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Geek</span> F. x </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <style>img{width:100%}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">关于 RNN 循环神经网络的反向传播求导</h1> <p class="post-meta"> Created in January 11, 2021 </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>本文是对 RNN 循环神经网络中的每一个神经元进行反向传播求导的数学推导过程，下面还使用 <code class="language-plaintext highlighter-rouge">PyTorch</code> 对导数公式进行编程求证。</p> <h2 id="rnn-神经网络架构">RNN 神经网络架构</h2> <p>一个普通的 RNN 神经网络如下图所示：</p> <p><img src="https://tvax1.sinaimg.cn/large/006VTcCxly1gmfae8mswmj317a0e00u2.jpg" alt="图片1"></p> <p>其中 \(x^{\langle t \rangle}\) 表示某一个输入数据在 \(t\) 时刻的输入；\(a^{\langle t \rangle}\) 表示神经网络在 \(t\) 时刻时的<em>hidden state</em>，也就是要传送到 \(t+1\) 时刻的值；\(y^{\langle t \rangle}\) 则表示在第 \(t\) 时刻输入数据传入以后产生的预测值，在进行预测或 <em>sampling</em> 时 \(y^{\langle t \rangle}\) 通常作为下一时刻即 \(t+1\) 时刻的输入，也就是说 \(x^{\langle t \rangle}=\hat{y}^{\langle t \rangle}\) ；下面对数据的维度进行说明。</p> <ul> <li>输入： \(x\in\mathbb{R}^{n_x\times m\times T_x}\) 其中 \(n_x\) 表示每一个时刻输入向量的长度；\(m\) 表示数据批量数（<em>batch</em>）；\(T_x\) 表示共有多少个输入的时刻（<em>time step</em>）。</li> <li>hidden state：\(a\in\mathbb{R}^{n_a\times m\times T_x}\) 其中 \(n_a\) 表示每一个 <em>hidden state</em> 的长度。</li> <li>预测：\(y\in\mathbb{R}^{n_y\times m\times T_y}\) 其中 \(n_y\) 表示预测输出的长度；\(T_y\) 表示共有多少个输出的时刻（<em>time step</em>）。</li> </ul> <h2 id="rnn-神经元">RNN 神经元</h2> <p>下图所示的是一个特定的 RNN 神经元：</p> <p><img src="https://tvax2.sinaimg.cn/large/006VTcCxly1gmfaesc2joj30z20didhj.jpg" alt="图片2"></p> <p>上图说明了在第 \(t\) 时刻的神经元中，数据的输入 \(x^{\langle t \rangle}\) 和上一层的 <em>hidden state</em> \(a^{\langle t \rangle}\) 是如何经过计算得到下一层的 <em>hidden state</em> 和预测输出 \(\hat{y}^{\langle t \rangle}\) 。</p> <p>下面是对五个参数的维度说明：</p> <ul> <li> \[W_{aa}\in\mathbb{R}^{n_a\times n_a}\] </li> <li> \[W_{ax}\in\mathbb{R}^{n_a\times n_x}\] </li> <li> \[b_a\in\mathbb{R}^{n_a\times 1}\] </li> <li> \[W_{ya}\in\mathbb{R}^{n_y\times n_a}\] </li> <li> \[b_y\in\mathbb{R}^{n_y\times 1}\] </li> </ul> <p>计算 \(t\) 时刻的 <em>hidden state</em> \(a^{\langle t \rangle}\) ：</p> \[\begin{split} z1^{\langle t \rangle} &amp;= W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a\\ a^{\langle t \rangle} &amp;= \tanh(z1^{\langle t \rangle}) \end{split}\] <p>预测 \(t\) 时刻的输出 \(\hat{y}^{\langle t \rangle}\) ：</p> \[\begin{split} z2^{\langle t \rangle} &amp;= W_{ya} a^{\langle t \rangle} + b_y\\ \hat{y}^{\langle t \rangle} &amp;= softmax(z2^{\langle t \rangle}) = \frac{e^{z2^{\langle t \rangle}}}{\sum_{i=1}^{n_y}e^{z2_i^{\langle t \rangle}}} \end{split}\] <h2 id="rnn-循环神经网络反向传播">RNN 循环神经网络反向传播</h2> <p>在当今流行的深度学习编程框架中，我们只需要编写一个神经网络的结构和负责神经网络的前向传播，至于反向传播的求导和参数更新，完全由框架搞定；即便如此，我们在学习阶段也要自己动手证明一下反向传播的有效性。</p> <h3 id="rnn-神经元的反向传播">RNN 神经元的反向传播</h3> <p>下图是 RNN 神经网络中的一个基本的神经元，图中标注了反向传播所需传来的参数和输出等。</p> <p><img src="https://tva4.sinaimg.cn/large/006VTcCxly1gmfagctqx4j30qu0e9t96.jpg" alt="图片3"></p> <p>就如一个全连接的神经网络一样，损失函数 \(J\) 的导数通过微积分的链式法则（<em>chain rule</em>）反向传播到每一个时间轴上。</p> <p>为了方便，我们将损失函数关于神经元中参数的偏导符号简记为 \(\mathrm{d}\mathit{parameters}\) ；例如将 \(\frac{\partial J}{\partial W_{ax}}\) 记为 \(\mathrm{d}W_{ax}\) 。</p> <p><img src="https://tva4.sinaimg.cn/large/006VTcCxly1gmfagmbbw5j30zk0k0ta2.jpg" alt="图片4"></p> <p>上图的反向传播的实现并没有包括全连接层和 <em>Softmax</em> 层。</p> <h3 id="反向传播求导">反向传播求导</h3> <p>计算损失函数关于各个参数的偏导数之前，我们先引入一个计算图（<em>computation graph</em>），其演示了一个 RNN 神经元的前向传播和如何利用计算图进行链式法则的反向求导。</p> <p><img src="https://tva1.sinaimg.cn/large/006VTcCxly1gmiwwe017fj31ia0j1go0.jpg" alt="image"></p> <p>因为当进行反向传播求导时，我们需要将整个时间轴的输入全部输入之后，才可以从最后一个时刻开始往前传进行反向传播，所以我们假设 \(t\) 时刻就为最后一个时刻 \(T_x\) 。</p> <p>如果我们想要先计算 \(\frac{\partial\ell}{\partial W_{ax}}\) 所以我们可以从计算图中看到，反向传播的路径：</p> <p><img src="https://tva1.sinaimg.cn/large/006VTcCxly1gmiwutfx3gj31ig0j1mzn.jpg" alt="image"></p> <p>我们需要按部就班的分别对从 \(W_{ax}\) 计算到 \(\ell\) 一路相关的变量进行求偏导，利用链式法则，将红色路线上一路的偏导数相乘到一起，就可以求出偏导数 \(\frac{\partial\ell}{\partial W_{ax}}\) ；所以我们得到：</p> \[\begin{split} \frac{\partial\ell}{\partial W_{ax}} &amp;= \frac{\partial\ell}{\partial\ell^{\langle t\rangle}} {\color{Red}{ \frac{\partial\ell^{\langle t\rangle}}{\partial\hat{y}^{\langle t\rangle}} \frac{\partial\hat{y}^{\langle t\rangle}}{\partial z2^{\langle t\rangle}} }} \frac{\partial z2^{\langle t\rangle}}{\partial a^{\langle t\rangle}} \frac{\partial a^{\langle t\rangle}}{\partial z1^{\langle t\rangle}} \frac{\partial z1^{\langle t\rangle}}{\partial W_{ax}} \end{split}\] <p>在上面的公式中，我们仅需要分别求出每一个偏导即可，其中红色的部分就是关于 \(\mathrm{Softmax}\) 的求导，关于 \(\mathrm{Softmax}\) 求导的推导过程，可以看本人的另一篇博客： <a href="/blog/2020/%E5%85%B3%E4%BA%8E-Softmax-%E5%9B%9E%E5%BD%92%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B1%82%E5%AF%BC%E6%95%B0%E8%BF%87%E7%A8%8B/">关于 Softmax 回归的反向传播求导数过程</a></p> <p>关于 \(\mathrm{tanh}\) 的求导公式如下：</p> \[\frac{\partial \tanh(x)} {\partial x} = 1 - \tanh^2(x)\] <p>所以上面的式子就得到：</p> \[\begin{split} \frac{\partial\ell}{\partial W_{ax}} &amp;= \frac{\partial\ell}{\partial\ell^{\langle t\rangle}} {\color{Red}{ \frac{\partial\ell^{\langle t\rangle}}{\partial\hat{y}^{\langle t\rangle}} \frac{\partial\hat{y}^{\langle t\rangle}}{\partial z2^{\langle t\rangle}} }} \frac{\partial z2^{\langle t\rangle}}{\partial a^{\langle t\rangle}} \frac{\partial a^{\langle t\rangle}}{\partial z1^{\langle t\rangle}} \frac{\partial z1^{\langle t\rangle}}{\partial W_{ax}}\\ &amp;= {\color{Red}{ (\hat{y}^{\langle t\rangle}-y^{\langle t\rangle}) }} W_{ya} (1-\tanh^2(z1^{\langle t\rangle})) x^{\langle t\rangle} \end{split}\] <p>我们就可以得到在最后时刻 \(t\) 参数 \(W_{ax}\) 的偏导数。</p> <blockquote> <p>关于上面式子中的偏导数的计算，除了标量对矩阵的求导，在后面还包括了两个一个矩阵或向量对另一个矩阵或向量中的求导，实际上这是非常麻烦的一件事。</p> <p>比如在计算 \(\frac{\partial z1^{\langle t\rangle}}{\partial W_{ax}}\) 偏导数的时候，我们发现 \(z1^{\langle t\rangle}\) 是一个 \(\mathbb{R}^{n_a\times m}\) 的矩阵，而 \(W_{ax}\) 则是一个 \(\mathbb{R}^{n_a\times n_x}\) 的矩阵，这一项就是一个矩阵对另一个矩阵求偏导，如果直接对其求导我们将会得到一个四维的矩阵 \(\mathbb{R}^{n_a\times n_x\times n_a\times m}\) （<em>雅可比矩阵 Jacobian matrix</em>）；只不过这个高维矩阵中偏导数的值有很多 \(0\) 。</p> <p>在神经网络中，如果直接将这个高维矩阵直接生搬硬套进梯度下降里更新参数是不可行，因为我们需要得到的梯度是关于自变量同型的向量或矩阵而且我们还要处理更高维度的矩阵的乘法；所以我们需要将结果进行一定的处理得到我们仅仅需要的信息。</p> <p>一般在深度学习框架中都会有自动求梯度的功能包，这些包（比如 <code class="language-plaintext highlighter-rouge">PyTorch</code> ）中就只允许一个标量对向量或矩阵求导，其他情况是不允许的，除非在反向传播的函数里传入一个同型的权重向量或矩阵才可以得到导数。</p> </blockquote> <p>我们先简单求出一个偏导数 \(\frac{\partial\ell}{\partial W_{ax}}\) 我们下面使用 <code class="language-plaintext highlighter-rouge">PyTorch</code> 中的自动求梯度的包进行验证我们的公式是否正确。</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 这是神经网络中的一些架构的参数
</span><span class="n">n_x</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">n_y</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">T_x</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">T_y</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_a</span> <span class="o">=</span> <span class="mi">3</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 定义所有参数矩阵
# requires_grad 为 True 表明在涉及这个变量的运算时建立计算图
# 为了之后反向传播求导
</span><span class="n">W_ax</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_a</span><span class="p">,</span> <span class="n">n_x</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">W_aa</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_a</span><span class="p">,</span> <span class="n">n_a</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ba</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_a</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">W_ya</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_y</span><span class="p">,</span> <span class="n">n_a</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">by</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># t 时刻的输入和上一时刻的 hidden state
</span><span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_x</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">a_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_a</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n_y</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 开始模拟一个神经元 t 时刻的前向传播
# 从输入一直到计算出 loss
</span><span class="n">z1_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">W_ax</span><span class="p">,</span> <span class="n">x_t</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">W_aa</span><span class="p">,</span> <span class="n">a_prev</span><span class="p">)</span> <span class="o">+</span> <span class="n">ba</span>
<span class="n">z1_t</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
<span class="n">a_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">z1_t</span><span class="p">)</span>
<span class="n">a_t</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
<span class="n">z2_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">W_ya</span><span class="p">,</span> <span class="n">a_t</span><span class="p">)</span> <span class="o">+</span> <span class="n">by</span>
<span class="n">z2_t</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">z2_t</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">z2_t</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_hat</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
<span class="n">loss_t</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y_t</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">loss_t</span><span class="p">.</span><span class="nf">retain_grad</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 对最后的 loss 标量开始进行反向传播求导
</span><span class="n">loss_t</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 我们就可以得到 W_ax 的导数
# 存储在后缀 _autograd 变量中，表明是由框架自动求导得到的
</span><span class="n">W_ax_autograd</span> <span class="o">=</span> <span class="n">W_ax</span><span class="p">.</span><span class="n">grad</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 查看框架计算得到的导数
</span><span class="n">W_ax_autograd</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0.5252,  1.1938, -0.2352,  1.1571, -1.0168,  0.3195],
        [-1.0536, -2.3949,  0.4718, -2.3213,  2.0398, -0.6410],
        [-0.0316, -0.0717,  0.0141, -0.0695,  0.0611, -0.0192]])
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 我们对自己推演出的公式进行手动计算导数
# 存储在后缀 _manugrad 变量中，表明是手动由公式计算得到的
</span><span class="n">W_ax_manugrad</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">((</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y_t</span><span class="p">).</span><span class="n">T</span><span class="p">,</span> <span class="n">W_ya</span><span class="p">).</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">z1_t</span><span class="p">))),</span> <span class="n">x_t</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="c1">#torch.matmul(torch.matmul(W_ya.T, y_hat - y_t) * (1 - torch.square(torch.tanh(z1_t))), x_t.T)
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 输出手动计算的导数
</span><span class="n">W_ax_manugrad</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0.5195,  1.1809, -0.2327,  1.1447, -1.0058,  0.3161],
        [-1.0195, -2.3172,  0.4565, -2.2461,  1.9737, -0.6202],
        [-0.0309, -0.0703,  0.0138, -0.0681,  0.0599, -0.0188]],
       grad_fn=&lt;MmBackward&gt;)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 查看两种求导结果的之差的 L2 范数
</span><span class="n">torch</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">W_ax_manugrad</span> <span class="o">-</span> <span class="n">W_ax_autograd</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(0.1356, grad_fn=&lt;CopyBackwards&gt;)
</code></pre></div></div> <p>通过上面的编程输出可以看到，我们手动计算的导数和框架自己求出的导数虽然有一定的误差，但是一一对照可以大体看到我们手动求出来的导数大体是对的，并没有说错的非常离谱。</p> <p>但上面只是当 \(t=T_x\) 即 \(t\) 时刻是最后一个输入单元的时候，也就是说所求的关于 \(_W{ax}\) 的导数只是全部导数的一部分，因为参数共享，所以每一时刻的神经元都有对 \(W_{ax}\) 的导数，所以需要将所有时刻的神经元关于 \(W_{ax}\) 的导数全部加起来。</p> <p>若 \(t\) 不是最后一时刻，可能是神经网络里的中间的某一时刻的神经元；也就是说，在进行反向传播的时候，想要求 \(t\) 时刻的导数，就得等到 \(t+1\) 时刻的导数值传进来，然后根据链式法则才可以计算当前时刻参数的导数。</p> <p>下面是一个简易的计算图，只绘制出了 \(W_ax\) 到 \(\ell\) 的计算中，共涉及到哪些变量（在整个神经网络中的 \(W_{ax}\) 的权重参数是共享的）：</p> <p><img src="https://tva2.sinaimg.cn/large/006VTcCxly1gmizn9a86aj318y0t8goe.jpg" alt="image"></p> <p>下面使用一个视频展示整个神经网络中从 \(W_{ax}\) 到一个数据批量的损失值 \(\ell\) 的大体流向：</p> <p><img src="https://yun.zyxweb.cn/index.php?explorer/share/file&amp;hash=1439457NtxPsb8asnr_vVtY66j-3v_8NDjbXDkWQTo-Tq5zESZQQZxsY&amp;name=forward.mp4" alt="forward.mp4"></p> <p>计算完 \(\ell\) 之后就可以计算 \(\frac{\partial\ell}{\partial W_{ax}}\) 的导数值，但是 RNN 神经网络的反向传播区别于全连接神经网络的。</p> <p><img src="https://tvax2.sinaimg.cn/large/006VTcCxly1gmj0k2u79mj31980swacc.jpg" alt="image"></p> <p>然后，我们演示一下如何进行反向传播的，注意看每一个时刻的 \(a^{\langle t\rangle}\) 的计算都是等 \(a^{\langle t+1\rangle}\) 的导数值传进来才进行计算的；同样地，\(W_{ax}\) 导数的计算也不是一步到位的，也是需要等到所有时刻的 \(a\) 的值全部传到才计算完。</p> <p><img src="https://yun.zyxweb.cn/index.php?explorer/share/file&amp;hash=1fc3U3bwTyfi-h40ykZa-0dfBrIcvkwjhXDDD_fGLGyO7xj52MiHSxWa&amp;name=backward.mp4" alt="backward.mp4"></p> <p>所以对于神经网络中间某一个单元 \(t\) 我们有：</p> \[\begin{split} \frac{\partial\ell}{\partial W_{ax}} &amp;= {\color{Red}{ \left( \frac{\partial\ell}{\partial a^{\langle t\rangle}} +\frac{\partial\ell}{\partial z1^{\langle t+1\rangle}} \frac{\partial z1^{\langle t+1\rangle}}{\partial a^{\langle t\rangle}} \right) }} \frac{\partial a^{\langle t\rangle}}{\partial z1^{\langle t\rangle}} \frac{\partial z1^{\langle t\rangle}}{\partial W_{ax}} \end{split}\] <p>关于红色的部分的意思是需要等到 \(t+1\) 时刻的导数值传进来，然后才可以进行对 \(t+1\) 时刻关于当前时刻 \(t\) 的参数求导，最后得到参数梯度的一个分量。其实若仔细展开每一个偏导项，就像是一个递归一样，每次求某一时刻的导数总是要从最后一时刻往前传到当前时刻才可以进行。</p> <blockquote> <p><strong>多元复合函数的求导法则</strong></p> <p>如果函数 \(u=\varphi(t)\) 及 \(v=\psi(t)\) 都在点 \(t\) 可导，函数 \(z=f(u,v)\) 在对应点 \((u,v)\) 具有连续偏导数，那么复合函数 \(z=f[\varphi(t),\psi(t)]\) 在点 \(t\) 可导，且有 \(\frac{\mathrm{d}z}{\mathrm{d}t}=\frac{\partial z}{\partial u}\frac{\mathrm{d}u}{\mathrm{d}t}+\frac{\partial z}{\partial v}\frac{\mathrm{d}v}{\mathrm{d}t}\)</p> </blockquote> <p>下面使用一张计算图说明 \(a^{\langle t\rangle}\) 到 \(\ell\) 的计算关系。</p> <p><img src="https://tvax4.sinaimg.cn/large/006VTcCxly1gmjsx4kqcqj30w80audg6.jpg" alt="image"></p> <p>也就是说第 \(t\) 时刻 \(\ell\) 关于 \(a^{\langle t\rangle}\) 的导数是由两部分相加组成，也就是说是由两条路径反向传播，这两条路径分别是 \(\ell\to\ell^{\langle t\rangle}\to\hat{y}^{\langle t\rangle}\to z2^{\langle t\rangle}\to a^{\langle t\rangle}\) 和 \(\ell\to\ell^{\langle t+1\rangle}\to\hat{y}^{\langle t+1\rangle}\to z2^{\langle t+1\rangle}\to a^{\langle t+1\rangle}\to z1^{\langle t+1\rangle}\to a^{\langle t\rangle}\) ，我们将这两条路径导数之和使用 \(\mathrm{d}a_{\mathrm{next}}\) 表示。</p> <p>所以我们可以得到在中间某一时刻的神经单元关于 \(W_{ax}\) 的导数为：</p> \[\frac{\partial\ell}{\partial W_{ax}}=\left(\mathrm{d}a_{\mathrm{next}} * \left( 1-\tanh^2(z1^{\langle t \rangle}\right)\right) x^{\langle t \rangle T}\] <p>通过同样的方法，我们就可以得到其它参数的导数：</p> \[\begin{align} \frac{\partial\ell}{\partial W_{aa}} &amp;= \left(\mathrm{d}a_{\mathrm{next}} * \left( 1-\tanh^2(z1^{\langle t\rangle}) \right)\right) a^{\langle t-1 \rangle T}\\ \frac{\partial\ell}{\partial b_a} &amp; = \sum_{batch}\left( da_{next} * \left( 1-\tanh^2(z1^{\langle t\rangle}) \right)\right)\\ \end{align}\] <p>除了传递参数的导数，在第 \(t\) 时刻还需要传送 \(\ell\) 关于 \(z1^{\langle t\rangle}\) 的导数到 \(t-1\) 时刻，将需要传送到上一时刻的导数记作为 \(\mathrm{d}a_{\mathrm{prev}}\) 我们得到：</p> \[\begin{split} \mathrm{d}a_{\mathrm{prev}} &amp;= \mathrm{d}a_\mathrm{next}\frac{\partial a^{\langle t\rangle}}{\partial z1^{\langle t\rangle}}\frac{\partial z1^{\langle t\rangle}}{\partial a^{\langle t-1\rangle}}\\ &amp;= { W_{aa}}^T\left(\mathrm{d}a_{\mathrm{next}} * \left( 1-\tanh^2(z1^{\langle t\rangle}) \right)\right) \end{split}\] <p>可以看到，一个循环神经网络的反向传播实际上是非常复杂的，因为每一时刻的神经元都与参数有计算关系，所以反向传播时的路径非常杂乱，其中还涉及到了高维的矩阵，所以在计算时需要对高维矩阵进行一定的矩阵代数转换才方便导数和更新参数的计算。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Ubuntu-22.04-%E5%AE%89%E8%A3%85-MySQL/">Ubuntu 22.04 安装 MySQL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Ubuntu-22.04-MacOS-Monterey-%E4%B8%BB%E9%A2%98/">Ubuntu 22.04 MacOS Monterey 主题</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/CASIA-WebMaskedFace-%E6%A8%A1%E6%8B%9F%E4%BD%A9%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E6%95%B0%E6%8D%AE%E9%9B%86/">CASIA-WebMaskedFace 模拟佩戴口罩人脸数据集</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/LaTeX-Workshop-%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF/">LaTeX Workshop 配置信息</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/%E5%85%B3%E4%BA%8ELinux%E4%B8%8BLaTeX%E6%97%A0%E6%B3%95%E6%89%BE%E5%88%B0%E5%B7%B2%E5%AE%89%E8%A3%85%E5%AD%97%E4%BD%93%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3/">关于Linux下LaTeX无法找到已安装字体的问题与解决</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Geek F. x. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>