<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 关于 Softmax 回归的反向传播求导数过程 | Geek F. x </title> <meta name="author" content="Geek F. x"> <meta name="description" content=""> <meta name="keywords" content="academic-website, portfolio-website, geek, geekfx, geekjkfx, jkfx, blog, computer-science, deep-learning, AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon/favicon_blue/favicon.ico?88a93ea07c0a82656ab29388cee125af"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jkfx.github.io/blog/2020/%E5%85%B3%E4%BA%8E-Softmax-%E5%9B%9E%E5%BD%92%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B1%82%E5%AF%BC%E6%95%B0%E8%BF%87%E7%A8%8B/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Geek</span> F. x </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <style>img{width:100%}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">关于 Softmax 回归的反向传播求导数过程</h1> <p class="post-meta"> Created in December 26, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>对于 \(Softmax\) 回归的正向传播非常简单，就是对于一个输入 \(X\) 对每一个输入标量 \(x_i\) 进行加权求和得到 \(Z\) 然后对其做概率归一化。</p> <h2 id="softmax-示意图">Softmax 示意图</h2> <p>下面看一个简单的示意图：</p> <p><img src="https://tva4.sinaimg.cn/large/006VTcCxgy1gm05gv92alj317s0b7t9d.jpg" alt="image"></p> <p>其中 \(X\in\mathbb{R}^{n\times m}\) 是一个向量或矩阵，这取决于传入的是一个训练样本还是一组训练样本，其中 \(n\) 是输入特征的数量，\(m\) 是传入的训练样本数量；此图只是示意的一个简单的 \(Softmax\) 的传播单元，可以把它理解为一个神经单元，所以 \(W\in\mathbb{R}^{n\times k}\) 可以看成每一个 \(X\) 的特征的权重，\(W\) 是向量还是矩阵同样取决于第二层（图中第一个方框）有多少个神经单元，用 \(k\) 表示第二层的数量，\(b\in\mathbb{R}\) 为偏置（bias）单元。</p> <h2 id="全连接神经网络">全连接神经网络</h2> <p>上图只是一个广泛的 \(Softmax\) 的示意图，下面用一个神经网络表示。</p> <p><img src="https://tva1.sinaimg.cn/large/006VTcCxly1gm0hr5c0mqj31e80q80we.jpg" alt="image"></p> <p>上图是更广义的 \(L\) 层全连接神经网络，其中，\(l_1\) 表示第一层的神经元数量，\(l_L\) 表示最后一层，即第 \(L\) 层的神经元数量，根据 \(Softmax\) 模型，假设此神经网络用作 \(C\) 分类的网络，\(l_L\) 的数量也是 \(C\) 的数量；\(\hat{y}\in\mathbb{R}^{C\times m}\) 可以是向量也可以是矩阵，同样取决于是否对一组输入进行了 <strong>矢量化（vectorization）</strong> ，表示每一个样本的预测概率；最后将 \(\hat{y}\) 传入损失函数 \(\ell(y,\hat{y})\) 对其计算损失，公式如下：</p> \[\ell(y,\hat{y})=-\sum_{i=1}^Cy_i\ln{\hat{y}_i}\] <p>最后，定义该网络一组训练样本的 \(cost\) 损失函数：</p> \[J(W,b)=\frac{1}{m}\sum_{i=1}^m\ell(y^{(i)},\hat{y}^{(i)})\] <h2 id="反向传播求导推理">反向传播求导推理</h2> <p>下面对神经网络进行反向传播的求导推导。</p> <p>首先，神经网络前向传播的最后一个操作是计算单个样本上的损失度 \(\ell(y,\hat{y})\) 所以首先计算 \(\frac{\partial\ell}{\partial\hat{y}}\) 由于神经网络的最后一层 \(a^{[L]}\) 就是 \(\hat{y}\) 的预测输出，所以就是对 \(a^{[L]}\) 求导：</p> \[\begin{split} \frac{\partial\ell}{\partial a^{[L]}}&amp;=\frac{\partial}{\partial a^{[L]}}\left(-\sum_{i=1}^Cy_i\ln{\hat{y}_i}\right)\\ &amp;=\frac{\partial}{\partial a^{[L]}}\left(-(y_1\ln{\hat{y}_1}+y_2\ln{\hat{y}_2}+\dots+y_C\ln{\hat{y}_C})\right)\\ &amp;=\frac{\partial}{\partial a^{[L]}}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right) \end{split}\] <p>可以从公式中看到，\(a^{[L]}\in\mathbb{R}^{C\times 1}\) 是一个向量，而 \(\ell\in\mathbb{R}\) 则为一个标量，根据 <strong>标量对向量</strong> 求导的法则，可以得到：</p> \[\begin{split} \frac{\partial\ell}{\partial a^{[L]}}&amp;=\frac{\partial}{\partial a^{[L]}}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right)\\ &amp;=\begin{bmatrix} \frac{\partial}{\partial a^{[L]}_1}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right)&amp; \frac{\partial}{\partial a^{[L]}_2}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right)&amp; \dots&amp; \frac{\partial}{\partial a^{[L]}_C}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right) \end{bmatrix}\\ &amp;=\begin{bmatrix} -\frac{y_1}{a^{[L]}_1}&amp; -\frac{y_2}{a^{[L]}_2}&amp; \dots&amp; -\frac{y_C}{a^{[L]}_C}&amp; \end{bmatrix}\\ &amp;=-\frac{y}{a^{[L]}}\\ &amp;=-\frac{y}{\hat{y}} \end{split}\] <p>得到 \(\frac{\partial\ell}{\partial a^{[L]}}\) 后，下面就继续对 \(\frac{\partial\ell}{\partial z^{[L]}}\) 求偏导，因为 \(a\) 是关于 \(z\) 的函数，所以使用链式求导法则 \(\frac{\partial\ell}{\partial z^{[L]}}=\frac{\partial\ell}{\partial a^{[L]}}\frac{\partial a^{[L]}}{\partial z^{[L]}}\) 下面计算 \(\frac{\partial a^{[L]}}{\partial z^{[L]}}\) 又因为 \(a^{[L]},z^{[L]}\in\mathbb{R}^{C\times 1}\) 都是相同维度的向量，所以根据 <strong>向量对向量</strong> 求导的法则，可以得到：</p> \[\begin{split} \frac{\partial a^{[L]}}{\partial z^{[L]}}&amp;=\begin{bmatrix} \frac{\partial a^{[L]}}{\partial z^{[L]}_1}&amp; \frac{\partial a^{[L]}}{\partial z^{[L]}_2}&amp; \dots&amp; \frac{\partial a^{[L]}}{\partial z^{[L]}_C} \end{bmatrix} \end{split}\] <p>可以观察上式子中，\(z^{[L]}_i\in\mathbb{R}\) 是一个标量，\(a^{[L]}\) 为向量，所以使用 <strong>向量对向量</strong> 的求导法则：</p> \[\begin{split} \frac{\partial a^{[L]}}{\partial z^{[L]}}&amp;=\begin{bmatrix} \frac{\partial a^{[L]}}{\partial z^{[L]}_1}&amp; \frac{\partial a^{[L]}}{\partial z^{[L]}_2}&amp; \dots&amp; \frac{\partial a^{[L]}}{\partial z^{[L]}_C} \end{bmatrix}\\ &amp;=\begin{bmatrix} \frac{\partial}{\partial z^{[L]}_1}\left(\frac{e^{z^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}} \right)&amp; \frac{\partial}{\partial z^{[L]}_2}\left(\frac{e^{z^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}} \right)&amp; \dots&amp; \frac{\partial}{\partial z^{[L]}_C}\left(\frac{e^{z^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}} \right) \end{bmatrix} \end{split}\] <p>我们拿出来第一个元素 \(\frac{\partial}{\partial z_1^{[L]}}\left(\frac{e_z^{[L]}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)\) 对其研究，发现是 <strong>向量对标量</strong> 求导，我们将其展开：</p> \[\begin{split} \frac{\partial}{\partial z_1^{[L]}}\left(\frac{e^{z^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)&amp;=\begin{bmatrix} \frac{\partial}{\partial z^{[L]}_1}\left(\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)&amp; \frac{\partial}{\partial z^{[L]}_1}\left(\frac{e^{z_2^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)&amp; \dots&amp; \frac{\partial}{\partial z^{[L]}_1}\left(\frac{e^{z_C^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)&amp; \end{bmatrix} \end{split}\] <p>我们可以从这个式子中发现一个规律，在对 \(\frac{\partial a^{[L]}}{\partial z^{[L]}}\) 求导的展开式中，每一项都会有一个分母项 \(z_i^{[L]}\) 和分子的向量中的一个元素 \(e^{z_i^{[L]}}\) 相对应，分子中的其它项 \(e^{z_j^{[L]}}\) 就与之不对应；</p> <p>比如在第一个元素 \(\frac{\partial a^{[L]}_1}{\partial z^{[L]}_1}\) 中，\(a\) 和 \(z\) 的下标都相同，所以可以得到：</p> \[\begin{split} \frac{\partial a^{[L]}_1}{\partial z_1^{[L]}} &amp;=\frac{\partial}{\partial z_1^{[L]}}\left(\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z_i^{[L]}}}\right)\\ &amp;=\frac{(e^{z_1^{[L]}})'\sum_{i=1}^Ce^{z_i^{[L]}}-e^{z_1^{[L]}}(e^{z_1^{[L]}}+e^{z_2^{[L]}}+\dots+e^{z_C^{[L]}})'}{\left(\sum_{i=1}^Ce^{z_i^{[L]}}\right)^2}\\ &amp;=\frac{e^{z_1^{[L]}}\sum_{i=1}^Ce^{z_i^{[L]}}-e^{z_1^{[L]}}e^{z_1^{[L]}}}{\left(\sum_{i=1}^Ce^{z_i^{[L]}}\right)^2}\\ &amp;=\frac{e^{z_1^{[L]}}\sum_{i=1}^Ce^{z_i^{[L]}}}{(\sum_{i=1}^Ce^{z_i^{[L]}})^2}-\frac{e^{z_1^{[L]}}e^{z_1^{[L]}}}{(\sum_{i=1}^Ce^{z_i^{[L]}})^2}\\ &amp;=\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z_i^{[L]}}}-\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z_i^{[L]}}}\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z_i^{[L]}}}\\ &amp;=a^{[L]}_1(1-a^{[L]}_1) \end{split}\] <p>我们可以将其对推广到其它的求导式子中，即当 \(i=j\) 时，我们可以得到：</p> \[\frac{\partial e^{z^{[L]}_i}}{\partial z^{[L]}_j}=a^{[L]}_i(1-a^{[L]}_i)\] <p>如果，当 \(i\neq j\) 时，我们得到（由于使用的 \(i,j\) 作为下标，故将分母的 \(\Sigma\) 累加和的下标使用 \(k\) 替换）：</p> \[\begin{split} \frac{\partial a^{[L]}_i}{\partial z_j^{[L]}} &amp;=\frac{\partial}{\partial z_j^{[L]}}\left(\frac{e^{z_i^{[L]}}}{\sum_{k=1}^Ce^{z_k^{[L]}}}\right)\\ &amp;=\frac{(e^{z_i^{[L]}})'\sum_{k=1}^Ce^{z_k^{[L]}}-e^{z_i^{[L]}}(e^{z_1^{[L]}}+e^{z_2^{[L]}}+\dots+e^{z_C^{[L]}})'}{\left(\sum_{k=1}^Ce^{z_k^{[L]}}\right)^2}\\ &amp;=\frac{0\sum_{i=1}^Ce^{z_i^{[L]}}-e^{z_i^{[L]}}e^{z_j^{[L]}}}{\left(\sum_{k=1}^Ce^{z_k^{[L]}}\right)^2}\\ &amp;=\frac{-e^{[L]}_ie^{[L]}_j}{(\sum_{k=1}^Ce^{[L]}_k)^2}\\ &amp;=-\frac{e^{[L]}_i}{\sum_{k=1}^Ce^{[L]}_k}\frac{e^{[L]}_j}{\sum_{k=1}^Ce^{[L]}_k}\\ &amp;=-a_ia_j \end{split}\] <p>然后我们写出 \(\frac{\partial a^{[L]}}{\partial z^{[L]}}\) 的 <strong>雅可比矩阵（jacobian matrix）</strong> ：</p> \[\begin{split} \frac{\partial a^{[L]}}{\partial z^{[L]}}&amp;=\begin{bmatrix} \frac{\partial a^{[L]}_1}{\partial z^{[L]}_1}&amp;\frac{\partial a^{[L]}_1}{\partial z^{[L]}_2}&amp;\dots&amp;\frac{\partial a^{[L]}_1}{\partial z^{[L]}_C}\\ \frac{\partial a^{[L]}_2}{\partial z^{[L]}_1}&amp;\frac{\partial a^{[L]}_2}{\partial z^{[L]}_2}&amp;\dots&amp;\frac{\partial a^{[L]}_2}{\partial z^{[L]}_C}\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ \frac{\partial a^{[L]}_C}{\partial z^{[L]}_1}&amp;\frac{\partial a^{[L]}_C}{\partial z^{[L]}_2}&amp;\dots&amp;\frac{\partial a^{[L]}_C}{\partial z^{[L]}C} \end{bmatrix}\\ \end{split}\] <p>我们发现除了对角线上即 \(i=j\) 时的求导为 \(a_i(1-a_j)\) 而矩阵的其它元素即 \(i\neq j\) 时求导为 \(-a_ia_j\) 。</p> <p>至此，我们求出来了 \(\frac{\partial\ell}{\partial a^{[L]}}\) 和 \(\frac{\partial a^{[L]}}{\partial z^{[L]}}\) 所以下面我们就开始计算 \(\frac{\partial\ell}{\partial z^{[L]}}\) 的导数：</p> \[\frac{\partial\ell}{\partial z^{[L]}}=\frac{\partial\ell}{\partial a^{[L]}}\frac{\partial a^{[L]}}{\partial z^{[L]}}\] <p>首先我们先看第一项，在上面我们已经求出了 \(\frac{\partial\ell}{\partial a^{[L]}}\) 的导数即 \(-\frac{y}{a^{[L]}}\) 首先我们从分子和分母中可以看到 \(y,a^{[L]}\in\mathbb{R}^{1\times C}\) 两个都是一个 \(1\times C\) 的向量，所以可以得到 \(\frac{\partial\ell}{\partial a^{[L]}}\) 也是一个 \(1\times C\) 向量；而式子的第二项我们刚刚求出了其 <strong>雅可比矩阵</strong> \(\frac{\partial a^{[L]}}{\partial z^{[L]}}\in\mathbb{R}^{C\times C}\) 是一个 \(C\times C\) 的矩阵，而在对 \(\frac{\partial\ell}{\partial z^{[L]}}\) 将向量和矩阵相乘，所以我们得到了一个 \(1\times C\) 的向量：</p> \[\begin{split} \frac{\partial\ell}{\partial a^{[L]}}\frac{\partial a^{[L]}}{\partial z^{[L]}}&amp;= \begin{bmatrix} -\frac{y_1}{a^{[L]}_1}&amp;-\frac{y_2}{a^{[L]}_2}&amp;\dots&amp;-\frac{y_C}{a^{[L]}_C} \end{bmatrix} \begin{bmatrix} \frac{\partial a^{[L]}_1}{\partial z^{[L]}_1}&amp;\frac{\partial a^{[L]}_1}{\partial z^{[L]}_2}&amp;\dots&amp;\frac{\partial a^{[L]}_1}{\partial z^{[L]}_C}\\ \frac{\partial a^{[L]}_2}{\partial z^{[L]}_1}&amp;\frac{\partial a^{[L]}_2}{\partial z^{[L]}_2}&amp;\dots&amp;\frac{\partial a^{[L]}_2}{\partial z^{[L]}_C}\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ \frac{\partial a^{[L]}_C}{\partial z^{[L]}_1}&amp;\frac{\partial a^{[L]}_C}{\partial z^{[L]}_2}&amp;\dots&amp;\frac{\partial a^{[L]}_C}{\partial z^{[L]}C} \end{bmatrix}\\ &amp;=\begin{bmatrix} -\frac{y_1}{a^{[L]}_1}\frac{\partial a^{[L]}_1}{\partial z^{[L]}_1}-\frac{y_2}{a^{[L]}_2}\frac{\partial a^{[L]}_2}{\partial z^{[L]}_1}-\dots-\frac{y_C}{a^{[L]}_C}\frac{\partial a^{[L]}_C}{\partial z^{[L]}_1}\\ -\frac{y_1}{a^{[L]}_1}\frac{\partial a^{[L]}_1}{\partial z^{[L]}_2}-\frac{y_2}{a^{[L]}_2}\frac{\partial a^{[L]}_2}{\partial z^{[L]}_2}-\dots-\frac{y_C}{a^{[L]}_C}\frac{\partial a^{[L]}_C}{\partial z^{[L]}_2}\\ \vdots\\ -\frac{y_1}{a^{[L]}_1}\frac{\partial a^{[L]}_1}{\partial z^{[L]}_C}-\frac{y_2}{a^{[L]}_2}\frac{\partial a^{[L]}_2}{\partial z^{[L]}_C}-\dots-\frac{y_C}{a^{[L]}_C}\frac{\partial a^{[L]}_C}{\partial z^{[L]}_C}\\ \end{bmatrix}^T\\ &amp;=\begin{bmatrix} -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_1}&amp; -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_2}&amp; \dots&amp; -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_C} \end{bmatrix} \end{split}\] <p>注意第二行得到的结果应该是一个 \(1\times C\) 的向量，由于排版所以将其转置成 \(C\times 1\) 的向量，不过这毫不影响推导。</p> <p>所以，根据式子的最后一步，我们可以得到，对于 \(a^{[L]}\) 上的所有元素 \(a^{[L]}_j\) 我们可以得到一个更统一化的式子：</p> \[\begin{split} \frac{\partial\ell}{\partial a^{[L]}}\frac{\partial a^{[L]}}{\partial z^{[L]}}&amp;=\begin{bmatrix} -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial a^{[L]}_1}&amp; -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial a^{[L]}_2}&amp; \dots&amp; -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial a^{[L]}_C} \end{bmatrix}\\ &amp;=\begin{bmatrix} -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial a^{[L]}_j} \end{bmatrix},j=[1,2,\dots,C]\\ \end{split}\] <p>我们观察式子的最后一项关于 \(\frac{\partial a^{[L]}_i}{\partial z^{[L]}_j}\) 我们在上面求出了其导数有两种情况：</p> \[\begin{split} \frac{\partial a^{[L]}_i}{\partial z^{[L]}_j}=\left\{\begin{matrix} a_i(1-a_j)&amp;&amp;i=j\\ -a_ia_j&amp;&amp;i\neq j \end{matrix}\right. \end{split}\] <p>我们将这个结果带回到上面的式子中：</p> \[\begin{split} -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_j}&amp;= \left\{\begin{matrix} -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}a^{[L]}_i(1-a^{[L]}_j)&amp;&amp;i=j\\ \sum_{i=1}^C\frac{y_i}{a^{[L]}_i}a^{[L]}_ia^{[L]}_j&amp;&amp;i\neq j \end{matrix}\right.\\ &amp;= \left\{\begin{matrix} -\sum_{i=1}^Cy_i(1-a^{[L]}_j)&amp;&amp;i=j\\ \sum_{i=1}^Cy_ia^{[L]}_j&amp;&amp;i\neq j \end{matrix}\right.\\ &amp;= \left\{\begin{matrix} \sum_{i=1}^Cy_ia^{[L]}_j-y_i&amp;&amp;i=j\\ \sum_{i=1}^Cy_ia^{[L]}_j&amp;&amp;i\neq j \end{matrix}\right. \end{split}\] <p>注意虽然最后把式子推导成为两个部分，但是最原始的式子就是要把每一项 \(i\) 与 \(j\) 全部加起来，不过就是因为要区别对待 \(i,j\) 相不相等的情况，这里当 \(i=j\) 时只有一项，所以可以将前面的 \(\sum\) 符号去掉，然后把后面的 \(i\neq j\) 的项加起来，我们可以得到：</p> \[\begin{split} -\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_j}&amp;= {\color{red}{-y_j+y_ja^{[L]}_j}}+{\color{green}{\sum_{i\in{\{i|i\neq j\}}}y_ia^{[L]}_j}}\\ &amp;=-y_j+{\color{blue}{y_ja^{[L]}_j+\sum_{i\in{\{i|i\neq j\}}}y_ia^{[L]}_j}}\\ &amp;=-y_j+{\color{orange}{\sum_{i=1}^Cy_ia^{[L]}_j}}\\ &amp;=-y_j+a^{[L]}_j\sum_{i=1}^Cy_i\\ &amp;=a^{[L]}_i-y_j\\ &amp;=a^{[L]}-y \end{split}\] <p>首先说明一下式子的第一行，红色的部分是 \(i=j\) 的情况，只有一项，所以不需要用 \(\sum\) 符号表示，绿色部分是当 \(i\neq j\) 的情况；</p> <p>在第二行中，我们可以看到，在整个蓝色的式子中，第一项是 \(i=j\) 的情况，而后面是 \(i\neq j\) 的所有项加起来，我们可以发现第一项的 \(y_j\) 正好补充了后面的 \(\sum\) 求和的部分，所以将这两项合并就到了 \(\sum_{i=1}^C\) 的项；</p> <p>因为下标是 \(i\) 索引的，所以将常数项 \(a^{[L]}_j\) 提到前面来；此时观察后面的 \(\sum_{i=1}^Cy_i\) 项，根据 \(Softmax\) 多分类的情况，\(y\) 是由一个 \(1\) 和其它全 \(0\) 组成的，所以对 \(y\) 进行累加和，我们得到的是 \(1\)</p> <p>最后，整理下式子，我们就能得到 \(\frac{\partial\ell}{\partial z^{[L]}}\) 的导数为 \(a^{[L]}-y\)</p> <h2 id="总结">总结</h2> <p>\(Softmax\) 回归的激活部分，和使用 \(Sigmoid/ReLu\) 作为激活函数是有所不同的，因为在 \(Sigmoid/ReLu\) 中，每一个神经元计算得到 \(z\) 后不需要将其它神经元的 \(z\) 全部累加起来做概率的 <strong>归一化</strong> ；也就是说以往的 \(Sigmoid/ReLu\) 作为激活函数，每一个神经元由 \(z\) 计算 \(a\) 时是独立于其它的神经元的；所以在反向传播求导数的时候，我们就能发现当计算 \(\frac{\partial a}{\partial z}\) 的时候，不再是单独的一一对应的关系，而是像正向传播那样，将上一层的结果全部集成到每一个神经元上，下面的图中，红色箭头表示了 \(Softmax\) 和 \(Sigmoid/ReLu\) 的反向传播的路径的有所不同。</p> <p><img src="https://tva4.sinaimg.cn/large/006VTcCxly1gm15ngqdafj31e50ucwj6.jpg" alt="image"></p> <p>在上图中， \(Softmax\) 层的激活的反向传播，可以看到每一个 \(a^{[L]}_i\) 都回馈到了不同的 \(z^{[L]}_j\) 的神经元上；其中红色的线表示了 \(i=j\) 的情况，其它蓝色的线表明了 \(i\neq j\) 的情况，这也说明了为什么在 \(Softmax\) 里的求导中会出现两种情况；反观第一层中的 \(Sigmoid/ReLu\) 激活中，每一个对 \(z^{[1]}_i\) 的激活都是在本地的神经元中得到的，没有其它神经单元传入的情况，所以也没有复杂的分下标 \(i,j\) 讨论求导的情况。</p> <h2 id="参考博客">参考博客</h2> <ol> <li><a href="https://www.cnblogs.com/zhaopAC/p/9539118.html" rel="external nofollow noopener" target="_blank">https://www.cnblogs.com/zhaopAC/p/9539118.html</a></li> <li><a href="https://blog.csdn.net/xxuffei/article/details/90022008" rel="external nofollow noopener" target="_blank">https://blog.csdn.net/xxuffei/article/details/90022008</a></li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/LaTeX%E4%B8%8D%E5%A4%AA%E7%AE%80%E7%9F%AD%E7%9A%84%E4%BB%8B%E7%BB%8D/">LaTeX 不太简短的介绍</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Ubuntu-22.04-%E5%AE%89%E8%A3%85-MySQL/">Ubuntu 22.04 安装 MySQL</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Clean-Architecture/">Clean Architecture</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Ubuntu-22.04-MacOS-Monterey-%E4%B8%BB%E9%A2%98/">LaTeX Workshop 配置信息</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/CASIA-WebMaskedFace-%E6%A8%A1%E6%8B%9F%E4%BD%A9%E6%88%B4%E5%8F%A3%E7%BD%A9%E4%BA%BA%E8%84%B8%E6%95%B0%E6%8D%AE%E9%9B%86/">CASIA-WebMaskedFace 模拟佩戴口罩人脸数据集</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Geek F. x. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>