---
layout: post
title: 关于 Softmax 回归的反向传播求导数过程
date: 2020-12-26 12:52
description: 
tags: 
categories: 
---

对于 $$Softmax$$ 回归的正向传播非常简单，就是对于一个输入 $$X$$ 对每一个输入标量 $$x_i$$ 进行加权求和得到 $$Z$$ 然后对其做概率归一化。

## Softmax 示意图

下面看一个简单的示意图：

![image](https://tva4.sinaimg.cn/large/006VTcCxgy1gm05gv92alj317s0b7t9d.jpg)

其中 $$X\in\mathbb{R}^{n\times m}$$ 是一个向量或矩阵，这取决于传入的是一个训练样本还是一组训练样本，其中 $$n$$ 是输入特征的数量，$$m$$ 是传入的训练样本数量；此图只是示意的一个简单的 $$Softmax$$ 的传播单元，可以把它理解为一个神经单元，所以 $$W\in\mathbb{R}^{n\times k}$$ 可以看成每一个 $$X$$ 的特征的权重，$$W$$ 是向量还是矩阵同样取决于第二层（图中第一个方框）有多少个神经单元，用 $$k$$ 表示第二层的数量，$$b\in\mathbb{R}$$ 为偏置（bias）单元。

## 全连接神经网络

上图只是一个广泛的 $$Softmax$$ 的示意图，下面用一个神经网络表示。

![image](https://tva1.sinaimg.cn/large/006VTcCxly1gm0hr5c0mqj31e80q80we.jpg)

上图是更广义的 $$L$$ 层全连接神经网络，其中，$$l_1$$ 表示第一层的神经元数量，$$l_L$$ 表示最后一层，即第 $$L$$ 层的神经元数量，根据 $$Softmax$$ 模型，假设此神经网络用作 $$C$$ 分类的网络，$$l_L$$ 的数量也是 $$C$$ 的数量；$$\hat{y}\in\mathbb{R}^{C\times m}$$ 可以是向量也可以是矩阵，同样取决于是否对一组输入进行了 **矢量化（vectorization）** ，表示每一个样本的预测概率；最后将 $$\hat{y}$$ 传入损失函数 $$\ell(y,\hat{y})$$ 对其计算损失，公式如下：

$$
\ell(y,\hat{y})=-\sum_{i=1}^Cy_i\ln{\hat{y}_i}
$$

最后，定义该网络一组训练样本的 $$cost$$ 损失函数：

$$
J(W,b)=\frac{1}{m}\sum_{i=1}^m\ell(y^{(i)},\hat{y}^{(i)})
$$

## 反向传播求导推理

下面对神经网络进行反向传播的求导推导。

首先，神经网络前向传播的最后一个操作是计算单个样本上的损失度 $$\ell(y,\hat{y})$$ 所以首先计算 $$\frac{\partial\ell}{\partial\hat{y}}$$ 由于神经网络的最后一层 $$a^{[L]}$$ 就是 $$\hat{y}$$ 的预测输出，所以就是对 $$a^{[L]}$$ 求导：

$$
\begin{split}
\frac{\partial\ell}{\partial a^{[L]}}&=\frac{\partial}{\partial a^{[L]}}\left(-\sum_{i=1}^Cy_i\ln{\hat{y}_i}\right)\\
&=\frac{\partial}{\partial a^{[L]}}\left(-(y_1\ln{\hat{y}_1}+y_2\ln{\hat{y}_2}+\dots+y_C\ln{\hat{y}_C})\right)\\
&=\frac{\partial}{\partial a^{[L]}}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right)
\end{split}
$$

可以从公式中看到，$$a^{[L]}\in\mathbb{R}^{C\times 1}$$ 是一个向量，而 $$\ell\in\mathbb{R}$$ 则为一个标量，根据 **标量对向量** 求导的法则，可以得到：

$$
\begin{split}
\frac{\partial\ell}{\partial a^{[L]}}&=\frac{\partial}{\partial a^{[L]}}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right)\\
&=\begin{bmatrix}
\frac{\partial}{\partial a^{[L]}_1}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right)&
\frac{\partial}{\partial a^{[L]}_2}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right)&
\dots&
\frac{\partial}{\partial a^{[L]}_C}\left(-(y_1\ln{a^{[L]}_1}+y_2\ln{a^{[L]}_2}+\dots+y_C\ln{a^{[L]}_C})\right)
\end{bmatrix}\\
&=\begin{bmatrix}
-\frac{y_1}{a^{[L]}_1}&
-\frac{y_2}{a^{[L]}_2}&
\dots&
-\frac{y_C}{a^{[L]}_C}&
\end{bmatrix}\\
&=-\frac{y}{a^{[L]}}\\
&=-\frac{y}{\hat{y}}
\end{split}
$$

得到 $$\frac{\partial\ell}{\partial a^{[L]}}$$ 后，下面就继续对 $$\frac{\partial\ell}{\partial z^{[L]}}$$ 求偏导，因为 $$a$$ 是关于 $$z$$ 的函数，所以使用链式求导法则 $$\frac{\partial\ell}{\partial z^{[L]}}=\frac{\partial\ell}{\partial a^{[L]}}\frac{\partial a^{[L]}}{\partial z^{[L]}}$$ 下面计算 $$\frac{\partial a^{[L]}}{\partial z^{[L]}}$$ 又因为 $$a^{[L]},z^{[L]}\in\mathbb{R}^{C\times 1}$$ 都是相同维度的向量，所以根据 **向量对向量** 求导的法则，可以得到：

$$
\begin{split}
\frac{\partial a^{[L]}}{\partial z^{[L]}}&=\begin{bmatrix}
\frac{\partial a^{[L]}}{\partial z^{[L]}_1}&
\frac{\partial a^{[L]}}{\partial z^{[L]}_2}&
\dots&
\frac{\partial a^{[L]}}{\partial z^{[L]}_C}
\end{bmatrix}
\end{split}
$$

可以观察上式子中，$$z^{[L]}_i\in\mathbb{R}$$ 是一个标量，$$a^{[L]}$$ 为向量，所以使用 **向量对向量** 的求导法则：

$$
\begin{split}
\frac{\partial a^{[L]}}{\partial z^{[L]}}&=\begin{bmatrix}
\frac{\partial a^{[L]}}{\partial z^{[L]}_1}&
\frac{\partial a^{[L]}}{\partial z^{[L]}_2}&
\dots&
\frac{\partial a^{[L]}}{\partial z^{[L]}_C}
\end{bmatrix}\\
&=\begin{bmatrix}
\frac{\partial}{\partial z^{[L]}_1}\left(\frac{e^{z^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}} \right)&
\frac{\partial}{\partial z^{[L]}_2}\left(\frac{e^{z^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}} \right)&
\dots&
\frac{\partial}{\partial z^{[L]}_C}\left(\frac{e^{z^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}} \right)
\end{bmatrix}
\end{split}
$$

我们拿出来第一个元素 $$\frac{\partial}{\partial z_1^{[L]}}\left(\frac{e_z^{[L]}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)$$ 对其研究，发现是 **向量对标量** 求导，我们将其展开：

$$
\begin{split}
\frac{\partial}{\partial z_1^{[L]}}\left(\frac{e^{z^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)&=\begin{bmatrix}
\frac{\partial}{\partial z^{[L]}_1}\left(\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)&
\frac{\partial}{\partial z^{[L]}_1}\left(\frac{e^{z_2^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)&
\dots&
\frac{\partial}{\partial z^{[L]}_1}\left(\frac{e^{z_C^{[L]}}}{\sum_{i=1}^Ce^{z^{[L]}_i}}\right)&
\end{bmatrix}
\end{split}
$$

我们可以从这个式子中发现一个规律，在对 $$\frac{\partial a^{[L]}}{\partial z^{[L]}}$$ 求导的展开式中，每一项都会有一个分母项 $$z_i^{[L]}$$ 和分子的向量中的一个元素 $$e^{z_i^{[L]}}$$ 相对应，分子中的其它项 $$e^{z_j^{[L]}}$$ 就与之不对应；

比如在第一个元素 $$\frac{\partial a^{[L]}_1}{\partial z^{[L]}_1}$$ 中，$$a$$ 和 $$z$$ 的下标都相同，所以可以得到：

$$
\begin{split}
\frac{\partial a^{[L]}_1}{\partial z_1^{[L]}}
&=\frac{\partial}{\partial z_1^{[L]}}\left(\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z_i^{[L]}}}\right)\\
&=\frac{(e^{z_1^{[L]}})'\sum_{i=1}^Ce^{z_i^{[L]}}-e^{z_1^{[L]}}(e^{z_1^{[L]}}+e^{z_2^{[L]}}+\dots+e^{z_C^{[L]}})'}{\left(\sum_{i=1}^Ce^{z_i^{[L]}}\right)^2}\\
&=\frac{e^{z_1^{[L]}}\sum_{i=1}^Ce^{z_i^{[L]}}-e^{z_1^{[L]}}e^{z_1^{[L]}}}{\left(\sum_{i=1}^Ce^{z_i^{[L]}}\right)^2}\\
&=\frac{e^{z_1^{[L]}}\sum_{i=1}^Ce^{z_i^{[L]}}}{(\sum_{i=1}^Ce^{z_i^{[L]}})^2}-\frac{e^{z_1^{[L]}}e^{z_1^{[L]}}}{(\sum_{i=1}^Ce^{z_i^{[L]}})^2}\\
&=\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z_i^{[L]}}}-\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z_i^{[L]}}}\frac{e^{z_1^{[L]}}}{\sum_{i=1}^Ce^{z_i^{[L]}}}\\
&=a^{[L]}_1(1-a^{[L]}_1)
\end{split}
$$

我们可以将其对推广到其它的求导式子中，即当 $$i=j$$ 时，我们可以得到：

$$
\frac{\partial e^{z^{[L]}_i}}{\partial z^{[L]}_j}=a^{[L]}_i(1-a^{[L]}_i)
$$

如果，当 $$i\neq j$$ 时，我们得到（由于使用的 $$i,j$$ 作为下标，故将分母的 $$\Sigma$$ 累加和的下标使用 $$k$$ 替换）：

$$
\begin{split}
\frac{\partial a^{[L]}_i}{\partial z_j^{[L]}}
&=\frac{\partial}{\partial z_j^{[L]}}\left(\frac{e^{z_i^{[L]}}}{\sum_{k=1}^Ce^{z_k^{[L]}}}\right)\\
&=\frac{(e^{z_i^{[L]}})'\sum_{k=1}^Ce^{z_k^{[L]}}-e^{z_i^{[L]}}(e^{z_1^{[L]}}+e^{z_2^{[L]}}+\dots+e^{z_C^{[L]}})'}{\left(\sum_{k=1}^Ce^{z_k^{[L]}}\right)^2}\\
&=\frac{0\sum_{i=1}^Ce^{z_i^{[L]}}-e^{z_i^{[L]}}e^{z_j^{[L]}}}{\left(\sum_{k=1}^Ce^{z_k^{[L]}}\right)^2}\\
&=\frac{-e^{[L]}_ie^{[L]}_j}{(\sum_{k=1}^Ce^{[L]}_k)^2}\\
&=-\frac{e^{[L]}_i}{\sum_{k=1}^Ce^{[L]}_k}\frac{e^{[L]}_j}{\sum_{k=1}^Ce^{[L]}_k}\\
&=-a_ia_j
\end{split}
$$

然后我们写出 $$\frac{\partial a^{[L]}}{\partial z^{[L]}}$$ 的 **雅可比矩阵（jacobian matrix）** ：

$$
\begin{split}
\frac{\partial a^{[L]}}{\partial z^{[L]}}&=\begin{bmatrix}
\frac{\partial a^{[L]}_1}{\partial z^{[L]}_1}&\frac{\partial a^{[L]}_1}{\partial z^{[L]}_2}&\dots&\frac{\partial a^{[L]}_1}{\partial z^{[L]}_C}\\
\frac{\partial a^{[L]}_2}{\partial z^{[L]}_1}&\frac{\partial a^{[L]}_2}{\partial z^{[L]}_2}&\dots&\frac{\partial a^{[L]}_2}{\partial z^{[L]}_C}\\
\vdots&\vdots&\ddots&\vdots\\
\frac{\partial a^{[L]}_C}{\partial z^{[L]}_1}&\frac{\partial a^{[L]}_C}{\partial z^{[L]}_2}&\dots&\frac{\partial a^{[L]}_C}{\partial z^{[L]}C}
\end{bmatrix}\\
\end{split}
$$

我们发现除了对角线上即 $$i=j$$ 时的求导为 $$a_i(1-a_j)$$ 而矩阵的其它元素即 $$i\neq j$$ 时求导为 $$-a_ia_j$$ 。

至此，我们求出来了 $$\frac{\partial\ell}{\partial a^{[L]}}$$ 和 $$\frac{\partial a^{[L]}}{\partial z^{[L]}}$$ 所以下面我们就开始计算 $$\frac{\partial\ell}{\partial z^{[L]}}$$ 的导数：

$$
\frac{\partial\ell}{\partial z^{[L]}}=\frac{\partial\ell}{\partial a^{[L]}}\frac{\partial a^{[L]}}{\partial z^{[L]}}
$$

首先我们先看第一项，在上面我们已经求出了 $$\frac{\partial\ell}{\partial a^{[L]}}$$ 的导数即 $$-\frac{y}{a^{[L]}}$$ 首先我们从分子和分母中可以看到 $$y,a^{[L]}\in\mathbb{R}^{1\times C}$$ 两个都是一个 $$1\times C$$ 的向量，所以可以得到 $$\frac{\partial\ell}{\partial a^{[L]}}$$ 也是一个 $$1\times C$$ 向量；而式子的第二项我们刚刚求出了其 **雅可比矩阵** $$\frac{\partial a^{[L]}}{\partial z^{[L]}}\in\mathbb{R}^{C\times C}$$ 是一个 $$C\times C$$ 的矩阵，而在对 $$\frac{\partial\ell}{\partial z^{[L]}}$$ 将向量和矩阵相乘，所以我们得到了一个 $$1\times C$$ 的向量：

$$
\begin{split}
\frac{\partial\ell}{\partial a^{[L]}}\frac{\partial a^{[L]}}{\partial z^{[L]}}&=
\begin{bmatrix}
-\frac{y_1}{a^{[L]}_1}&-\frac{y_2}{a^{[L]}_2}&\dots&-\frac{y_C}{a^{[L]}_C}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial a^{[L]}_1}{\partial z^{[L]}_1}&\frac{\partial a^{[L]}_1}{\partial z^{[L]}_2}&\dots&\frac{\partial a^{[L]}_1}{\partial z^{[L]}_C}\\
\frac{\partial a^{[L]}_2}{\partial z^{[L]}_1}&\frac{\partial a^{[L]}_2}{\partial z^{[L]}_2}&\dots&\frac{\partial a^{[L]}_2}{\partial z^{[L]}_C}\\
\vdots&\vdots&\ddots&\vdots\\
\frac{\partial a^{[L]}_C}{\partial z^{[L]}_1}&\frac{\partial a^{[L]}_C}{\partial z^{[L]}_2}&\dots&\frac{\partial a^{[L]}_C}{\partial z^{[L]}C}
\end{bmatrix}\\
&=\begin{bmatrix}
-\frac{y_1}{a^{[L]}_1}\frac{\partial a^{[L]}_1}{\partial z^{[L]}_1}-\frac{y_2}{a^{[L]}_2}\frac{\partial a^{[L]}_2}{\partial z^{[L]}_1}-\dots-\frac{y_C}{a^{[L]}_C}\frac{\partial a^{[L]}_C}{\partial z^{[L]}_1}\\
-\frac{y_1}{a^{[L]}_1}\frac{\partial a^{[L]}_1}{\partial z^{[L]}_2}-\frac{y_2}{a^{[L]}_2}\frac{\partial a^{[L]}_2}{\partial z^{[L]}_2}-\dots-\frac{y_C}{a^{[L]}_C}\frac{\partial a^{[L]}_C}{\partial z^{[L]}_2}\\
\vdots\\
-\frac{y_1}{a^{[L]}_1}\frac{\partial a^{[L]}_1}{\partial z^{[L]}_C}-\frac{y_2}{a^{[L]}_2}\frac{\partial a^{[L]}_2}{\partial z^{[L]}_C}-\dots-\frac{y_C}{a^{[L]}_C}\frac{\partial a^{[L]}_C}{\partial z^{[L]}_C}\\
\end{bmatrix}^T\\
&=\begin{bmatrix}
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_1}&
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_2}&
\dots&
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_C}
\end{bmatrix}
\end{split}
$$

注意第二行得到的结果应该是一个 $$1\times C$$ 的向量，由于排版所以将其转置成 $$C\times 1$$ 的向量，不过这毫不影响推导。

所以，根据式子的最后一步，我们可以得到，对于 $$a^{[L]}$$ 上的所有元素 $$a^{[L]}_j$$ 我们可以得到一个更统一化的式子：

$$
\begin{split}
\frac{\partial\ell}{\partial a^{[L]}}\frac{\partial a^{[L]}}{\partial z^{[L]}}&=\begin{bmatrix}
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial a^{[L]}_1}&
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial a^{[L]}_2}&
\dots&
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial a^{[L]}_C}
\end{bmatrix}\\
&=\begin{bmatrix}
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial a^{[L]}_j}
\end{bmatrix},j=[1,2,\dots,C]\\
\end{split}
$$

我们观察式子的最后一项关于 $$\frac{\partial a^{[L]}_i}{\partial z^{[L]}_j}$$ 我们在上面求出了其导数有两种情况：

$$
\begin{split}
\frac{\partial a^{[L]}_i}{\partial z^{[L]}_j}=\left\{\begin{matrix}
a_i(1-a_j)&&i=j\\
-a_ia_j&&i\neq j
\end{matrix}\right.
\end{split}
$$

我们将这个结果带回到上面的式子中：

$$
\begin{split}
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_j}&=
\left\{\begin{matrix}
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}a^{[L]}_i(1-a^{[L]}_j)&&i=j\\
\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}a^{[L]}_ia^{[L]}_j&&i\neq j
\end{matrix}\right.\\
&=
\left\{\begin{matrix}
-\sum_{i=1}^Cy_i(1-a^{[L]}_j)&&i=j\\
\sum_{i=1}^Cy_ia^{[L]}_j&&i\neq j
\end{matrix}\right.\\
&=
\left\{\begin{matrix}
\sum_{i=1}^Cy_ia^{[L]}_j-y_i&&i=j\\
\sum_{i=1}^Cy_ia^{[L]}_j&&i\neq j
\end{matrix}\right.
\end{split}
$$

注意虽然最后把式子推导成为两个部分，但是最原始的式子就是要把每一项 $$i$$ 与 $$j$$ 全部加起来，不过就是因为要区别对待 $$i,j$$ 相不相等的情况，这里当 $$i=j$$ 时只有一项，所以可以将前面的 $$\sum$$ 符号去掉，然后把后面的 $$i\neq j$$ 的项加起来，我们可以得到：

$$
\begin{split}
-\sum_{i=1}^C\frac{y_i}{a^{[L]}_i}\frac{\partial a^{[L]}_i}{\partial z^{[L]}_j}&=
{\color{red}{-y_j+y_ja^{[L]}_j}}+{\color{green}{\sum_{i\in{\{i|i\neq j\}}}y_ia^{[L]}_j}}\\
&=-y_j+{\color{blue}{y_ja^{[L]}_j+\sum_{i\in{\{i|i\neq j\}}}y_ia^{[L]}_j}}\\
&=-y_j+{\color{orange}{\sum_{i=1}^Cy_ia^{[L]}_j}}\\
&=-y_j+a^{[L]}_j\sum_{i=1}^Cy_i\\
&=a^{[L]}_i-y_j\\
&=a^{[L]}-y
\end{split}
$$

首先说明一下式子的第一行，红色的部分是 $$i=j$$ 的情况，只有一项，所以不需要用 $$\sum$$ 符号表示，绿色部分是当 $$i\neq j$$ 的情况；

在第二行中，我们可以看到，在整个蓝色的式子中，第一项是 $$i=j$$ 的情况，而后面是 $$i\neq j$$ 的所有项加起来，我们可以发现第一项的 $$y_j$$ 正好补充了后面的 $$\sum$$ 求和的部分，所以将这两项合并就到了 $$\sum_{i=1}^C$$ 的项；

因为下标是 $$i$$ 索引的，所以将常数项 $$a^{[L]}_j$$ 提到前面来；此时观察后面的 $$\sum_{i=1}^Cy_i$$ 项，根据 $$Softmax$$ 多分类的情况，$$y$$ 是由一个 $$1$$ 和其它全 $$0$$ 组成的，所以对 $$y$$ 进行累加和，我们得到的是 $$1$$

最后，整理下式子，我们就能得到 $$\frac{\partial\ell}{\partial z^{[L]}}$$ 的导数为 $$a^{[L]}-y$$

## 总结

$$Softmax$$ 回归的激活部分，和使用 $$Sigmoid/ReLu$$ 作为激活函数是有所不同的，因为在 $$Sigmoid/ReLu$$ 中，每一个神经元计算得到 $$z$$ 后不需要将其它神经元的 $$z$$ 全部累加起来做概率的 **归一化** ；也就是说以往的 $$Sigmoid/ReLu$$ 作为激活函数，每一个神经元由 $$z$$ 计算 $$a$$ 时是独立于其它的神经元的；所以在反向传播求导数的时候，我们就能发现当计算 $$\frac{\partial a}{\partial z}$$ 的时候，不再是单独的一一对应的关系，而是像正向传播那样，将上一层的结果全部集成到每一个神经元上，下面的图中，红色箭头表示了 $$Softmax$$ 和 $$Sigmoid/ReLu$$ 的反向传播的路径的有所不同。

![image](https://tva4.sinaimg.cn/large/006VTcCxly1gm15ngqdafj31e50ucwj6.jpg)

在上图中， $$Softmax$$ 层的激活的反向传播，可以看到每一个 $$a^{[L]}_i$$ 都回馈到了不同的 $$z^{[L]}_j$$ 的神经元上；其中红色的线表示了 $$i=j$$ 的情况，其它蓝色的线表明了 $$i\neq j$$ 的情况，这也说明了为什么在 $$Softmax$$ 里的求导中会出现两种情况；反观第一层中的 $$Sigmoid/ReLu$$ 激活中，每一个对 $$z^{[1]}_i$$ 的激活都是在本地的神经元中得到的，没有其它神经单元传入的情况，所以也没有复杂的分下标 $$i,j$$ 讨论求导的情况。

## 参考博客

1. [https://www.cnblogs.com/zhaopAC/p/9539118.html](https://www.cnblogs.com/zhaopAC/p/9539118.html)
2. [https://blog.csdn.net/xxuffei/article/details/90022008](https://blog.csdn.net/xxuffei/article/details/90022008)
